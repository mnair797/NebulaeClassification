{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "MobileNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e24119f9",
        "outputId": "07b2c830-0752-47b4-b930-d10424d76d1d"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "id": "e24119f9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2db8596c"
      },
      "source": [
        "\n",
        "# Please replace the brackets below with the drive location of your folder which included subfolders for images\n",
        "# Sample path: /content/drive/My Drive/ImageClassification\n",
        "PATH = '/content/drive/My Drive/NebulasWithoutDarkReflection'"
      ],
      "id": "2db8596c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ddc56bf"
      },
      "source": [
        "def create_model(base_model, num_classes):\n",
        "    # Grab the last layer and add a few extra layers to it\n",
        "    x=base_model.output\n",
        "    x=GlobalAveragePooling2D()(x)\n",
        "    # Dense layer 1\n",
        "    x=tf.keras.layers.Dense(1024,activation='relu', kernel_initializer=tf.keras.initializers.VarianceScaling, use_bias=True)(x)\n",
        "\n",
        "    # Final layer with softmax activation\n",
        "    preds=tf.keras.layers.Dense(num_classes,activation='softmax', kernel_initializer=tf.keras.initializers.VarianceScaling, use_bias=False\n",
        "\n",
        ")(x) \n",
        "    \n",
        "    # Create the final model\n",
        "    model=Model(inputs=base_model.input,outputs=preds)\n",
        "    return model\n"
      ],
      "id": "2ddc56bf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ab6f509",
        "outputId": "9bc1a575-e183-46a2-a744-42fc93109a39"
      },
      "source": [
        "# Import packages needed to create a imaage classification model\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from keras.applications.resnet import preprocess_input\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense,GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "\n",
        "BATCH_SIZE = 40\n",
        "IMG_SIZE = (224, 224)\n",
        "\n",
        "# Create the data generation pipeline for training and validation\n",
        "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input, validation_split=0.2) # set validation split\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(PATH,\n",
        "                                                target_size=IMG_SIZE,\n",
        "                                                color_mode='rgb',\n",
        "                                                batch_size=BATCH_SIZE,\n",
        "                                                class_mode='categorical',\n",
        "                                                shuffle=True,\n",
        "                                                subset = 'training')\n",
        "validation_generator = train_datagen.flow_from_directory(PATH,\n",
        "                                                target_size=IMG_SIZE,\n",
        "                                                color_mode='rgb',\n",
        "                                                batch_size=BATCH_SIZE,\n",
        "                                                class_mode='categorical',\n",
        "                                                shuffle=False,\n",
        "                                                subset = 'validation')\n",
        "\n",
        "# Download the model\n",
        "base_model = tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable=False\n",
        "\"\"\"\n",
        "# If we want to set the first 20 layers of the network to be non-trainable\n",
        "for layer in base_model.layers[:150]:\n",
        "    layer.trainable=False\n",
        "for layer in base_model.layers[150:]:\n",
        "    layer.trainable=True\n",
        "\"\"\"\n",
        "\n",
        "# Specify the number of classes\n",
        "num_classes = 3\n",
        "\n",
        "# Create the base model\n",
        "model = create_model(base_model,num_classes)\n",
        "\n",
        "print(len(base_model.layers))\n",
        "\n",
        "base_learning_rate = 0.0001 #decrease for different results; use excel sheet to note down results from each change to learning rate and epochs\n",
        "\n",
        "opt = keras.optimizers.Adam(learning_rate=base_learning_rate)\n",
        "model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])# Adam optimizer\n",
        "# loss function will be categorical cross entropy\n",
        "# evaluation metric will be accuracy\n",
        "\n",
        "step_size_train=train_generator.n//train_generator.batch_size\n",
        "history_fine = model.fit(train_generator,\n",
        "                        steps_per_epoch=step_size_train,\n",
        "                        epochs=45, #<-- increase for higher accuracy\n",
        "                        validation_data = validation_generator)\n"
      ],
      "id": "5ab6f509",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 187 images belonging to 3 classes.\n",
            "Found 45 images belonging to 3 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n",
            "9420800/9406464 [==============================] - 0s 0us/step\n",
            "154\n",
            "Epoch 1/45\n",
            "4/4 [==============================] - 41s 7s/step - loss: 1.0801 - accuracy: 0.4354 - val_loss: 0.8760 - val_accuracy: 0.5333\n",
            "Epoch 2/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.8346 - accuracy: 0.7143 - val_loss: 0.7656 - val_accuracy: 0.6889\n",
            "Epoch 3/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.7357 - accuracy: 0.7551 - val_loss: 0.7158 - val_accuracy: 0.7333\n",
            "Epoch 4/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.6567 - accuracy: 0.7937 - val_loss: 0.6855 - val_accuracy: 0.6889\n",
            "Epoch 5/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.5997 - accuracy: 0.7687 - val_loss: 0.6869 - val_accuracy: 0.7556\n",
            "Epoch 6/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.5370 - accuracy: 0.8163 - val_loss: 0.6866 - val_accuracy: 0.7556\n",
            "Epoch 7/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.4690 - accuracy: 0.8776 - val_loss: 0.6520 - val_accuracy: 0.7333\n",
            "Epoch 8/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.4500 - accuracy: 0.8687 - val_loss: 0.6336 - val_accuracy: 0.7556\n",
            "Epoch 9/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.4525 - accuracy: 0.8571 - val_loss: 0.6300 - val_accuracy: 0.7556\n",
            "Epoch 10/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.3705 - accuracy: 0.9116 - val_loss: 0.6585 - val_accuracy: 0.7333\n",
            "Epoch 11/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.3615 - accuracy: 0.9125 - val_loss: 0.6699 - val_accuracy: 0.7556\n",
            "Epoch 12/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.3500 - accuracy: 0.9184 - val_loss: 0.6516 - val_accuracy: 0.7556\n",
            "Epoch 13/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.3224 - accuracy: 0.9184 - val_loss: 0.6328 - val_accuracy: 0.7778\n",
            "Epoch 14/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.2712 - accuracy: 0.9456 - val_loss: 0.6275 - val_accuracy: 0.7778\n",
            "Epoch 15/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.2947 - accuracy: 0.9184 - val_loss: 0.6406 - val_accuracy: 0.7556\n",
            "Epoch 16/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.2625 - accuracy: 0.9320 - val_loss: 0.6542 - val_accuracy: 0.7778\n",
            "Epoch 17/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.2435 - accuracy: 0.9500 - val_loss: 0.6387 - val_accuracy: 0.7778\n",
            "Epoch 18/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.2247 - accuracy: 0.9524 - val_loss: 0.6441 - val_accuracy: 0.7778\n",
            "Epoch 19/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.2233 - accuracy: 0.9524 - val_loss: 0.6462 - val_accuracy: 0.7778\n",
            "Epoch 20/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.1962 - accuracy: 0.9660 - val_loss: 0.6258 - val_accuracy: 0.7778\n",
            "Epoch 21/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.1968 - accuracy: 0.9728 - val_loss: 0.6259 - val_accuracy: 0.7778\n",
            "Epoch 22/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.1819 - accuracy: 0.9796 - val_loss: 0.6452 - val_accuracy: 0.7778\n",
            "Epoch 23/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.1679 - accuracy: 0.9796 - val_loss: 0.6643 - val_accuracy: 0.7778\n",
            "Epoch 24/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.1731 - accuracy: 0.9750 - val_loss: 0.6411 - val_accuracy: 0.7778\n",
            "Epoch 25/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.1705 - accuracy: 0.9864 - val_loss: 0.6221 - val_accuracy: 0.7778\n",
            "Epoch 26/45\n",
            "4/4 [==============================] - 6s 1s/step - loss: 0.1606 - accuracy: 0.9864 - val_loss: 0.6444 - val_accuracy: 0.7778\n",
            "Epoch 27/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.1437 - accuracy: 0.9796 - val_loss: 0.6503 - val_accuracy: 0.7778\n",
            "Epoch 28/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.1373 - accuracy: 0.9932 - val_loss: 0.6385 - val_accuracy: 0.7778\n",
            "Epoch 29/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.1345 - accuracy: 1.0000 - val_loss: 0.6315 - val_accuracy: 0.7778\n",
            "Epoch 30/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.1131 - accuracy: 1.0000 - val_loss: 0.6498 - val_accuracy: 0.7778\n",
            "Epoch 31/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.1160 - accuracy: 1.0000 - val_loss: 0.6500 - val_accuracy: 0.7778\n",
            "Epoch 32/45\n",
            "4/4 [==============================] - 6s 1s/step - loss: 0.1154 - accuracy: 1.0000 - val_loss: 0.6456 - val_accuracy: 0.7778\n",
            "Epoch 33/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.1037 - accuracy: 1.0000 - val_loss: 0.6504 - val_accuracy: 0.7778\n",
            "Epoch 34/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.0965 - accuracy: 1.0000 - val_loss: 0.6559 - val_accuracy: 0.7778\n",
            "Epoch 35/45\n",
            "4/4 [==============================] - 6s 1s/step - loss: 0.0913 - accuracy: 1.0000 - val_loss: 0.6665 - val_accuracy: 0.7778\n",
            "Epoch 36/45\n",
            "4/4 [==============================] - 5s 1s/step - loss: 0.0848 - accuracy: 1.0000 - val_loss: 0.6656 - val_accuracy: 0.7778\n",
            "Epoch 37/45\n",
            "4/4 [==============================] - 6s 1s/step - loss: 0.0872 - accuracy: 1.0000 - val_loss: 0.6627 - val_accuracy: 0.7778\n",
            "Epoch 38/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.0853 - accuracy: 1.0000 - val_loss: 0.6657 - val_accuracy: 0.7778\n",
            "Epoch 39/45\n",
            "4/4 [==============================] - 6s 1s/step - loss: 0.0764 - accuracy: 1.0000 - val_loss: 0.6753 - val_accuracy: 0.7778\n",
            "Epoch 40/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.0772 - accuracy: 1.0000 - val_loss: 0.6765 - val_accuracy: 0.7778\n",
            "Epoch 41/45\n",
            "4/4 [==============================] - 6s 1s/step - loss: 0.0656 - accuracy: 1.0000 - val_loss: 0.6785 - val_accuracy: 0.7778\n",
            "Epoch 42/45\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.0704 - accuracy: 1.0000 - val_loss: 0.6676 - val_accuracy: 0.7778\n",
            "Epoch 43/45\n",
            "4/4 [==============================] - 5s 1s/step - loss: 0.0694 - accuracy: 1.0000 - val_loss: 0.6708 - val_accuracy: 0.7778\n",
            "Epoch 44/45\n",
            "4/4 [==============================] - 6s 1s/step - loss: 0.0644 - accuracy: 1.0000 - val_loss: 0.6931 - val_accuracy: 0.7778\n",
            "Epoch 45/45\n",
            "4/4 [==============================] - 6s 1s/step - loss: 0.0564 - accuracy: 1.0000 - val_loss: 0.7067 - val_accuracy: 0.7778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db0888dc",
        "outputId": "3e2d6d8c-f741-49f3-fe41-f389d43e2abf"
      },
      "source": [
        "\n",
        "# Populating matrics -> accuracy & loss\n",
        "acc = history_fine.history['accuracy']\n",
        "val_acc = history_fine.history['val_accuracy']\n",
        "\n",
        "loss = history_fine.history['loss']\n",
        "val_loss = history_fine.history['val_loss']\n",
        "\n",
        "print('Training Accuracy: ', acc)\n",
        "print('Validation Accuracy: ', acc)\n",
        "print('Training Loss: ', loss)\n",
        "print('Validation Loss: ', val_loss)\n"
      ],
      "id": "db0888dc",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy:  [0.26696833968162537, 0.4615384638309479, 0.5927602052688599, 0.6742081642150879, 0.692307710647583, 0.7692307829856873, 0.7873303294181824, 0.8280543088912964, 0.8280543088912964, 0.8461538553237915, 0.8597285151481628, 0.8733031749725342, 0.8823529481887817, 0.8959276080131531, 0.8979591727256775, 0.9004524946212769, 0.9230769276618958, 0.918552041053772, 0.9321267008781433, 0.9366515874862671, 0.9457013607025146, 0.959276020526886, 0.9714285731315613, 0.9683257937431335, 0.9673469662666321, 0.9773755669593811, 0.9773755669593811, 0.9864253401756287, 0.9864253401756287, 0.9954751133918762, 0.9819004535675049, 0.9819004535675049, 0.9954751133918762, 0.9909502267837524, 0.9954751133918762, 0.9954751133918762, 0.9909502267837524, 0.9954751133918762, 0.9954751133918762, 0.9954751133918762, 0.9954751133918762, 0.9954751133918762, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "Validation Accuracy:  [0.26696833968162537, 0.4615384638309479, 0.5927602052688599, 0.6742081642150879, 0.692307710647583, 0.7692307829856873, 0.7873303294181824, 0.8280543088912964, 0.8280543088912964, 0.8461538553237915, 0.8597285151481628, 0.8733031749725342, 0.8823529481887817, 0.8959276080131531, 0.8979591727256775, 0.9004524946212769, 0.9230769276618958, 0.918552041053772, 0.9321267008781433, 0.9366515874862671, 0.9457013607025146, 0.959276020526886, 0.9714285731315613, 0.9683257937431335, 0.9673469662666321, 0.9773755669593811, 0.9773755669593811, 0.9864253401756287, 0.9864253401756287, 0.9954751133918762, 0.9819004535675049, 0.9819004535675049, 0.9954751133918762, 0.9909502267837524, 0.9954751133918762, 0.9954751133918762, 0.9909502267837524, 0.9954751133918762, 0.9954751133918762, 0.9954751133918762, 0.9954751133918762, 0.9954751133918762, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "Training Loss:  [1.6113964319229126, 1.3350439071655273, 1.1762771606445312, 1.0093408823013306, 0.9321728348731995, 0.8422346711158752, 0.7777843475341797, 0.7182988524436951, 0.6631535887718201, 0.6136696934700012, 0.5942766666412354, 0.5424306988716125, 0.5163266658782959, 0.48042500019073486, 0.46118712425231934, 0.4386650323867798, 0.405892014503479, 0.3933278024196625, 0.37292081117630005, 0.3422561287879944, 0.33376485109329224, 0.32106998562812805, 0.29257598519325256, 0.27661246061325073, 0.26681074500083923, 0.24844524264335632, 0.2377697080373764, 0.22728949785232544, 0.22095376253128052, 0.2021971493959427, 0.19168655574321747, 0.1960090845823288, 0.1663230061531067, 0.16877038776874542, 0.15859723091125488, 0.15131859481334686, 0.15296438336372375, 0.14957579970359802, 0.1356717050075531, 0.13560111820697784, 0.13259251415729523, 0.12400979548692703, 0.11518708616495132, 0.1095050647854805, 0.10320930927991867, 0.10045971721410751, 0.10055189579725266, 0.09184478968381882, 0.09080850332975388, 0.08728862553834915]\n",
            "Validation Loss:  [1.5461004972457886, 1.4331469535827637, 1.3496849536895752, 1.3404905796051025, 1.3082551956176758, 1.2860264778137207, 1.3067781925201416, 1.3020354509353638, 1.299816370010376, 1.260925531387329, 1.3121106624603271, 1.2876451015472412, 1.2648839950561523, 1.2688255310058594, 1.2541954517364502, 1.2909419536590576, 1.2760298252105713, 1.305305004119873, 1.263835072517395, 1.305407166481018, 1.2574992179870605, 1.2600102424621582, 1.2600157260894775, 1.3045639991760254, 1.2876988649368286, 1.3329797983169556, 1.3402363061904907, 1.2795459032058716, 1.3195661306381226, 1.3625266551971436, 1.3179036378860474, 1.3015027046203613, 1.3422234058380127, 1.3715637922286987, 1.3768478631973267, 1.374243974685669, 1.3782986402511597, 1.3573030233383179, 1.341485857963562, 1.371734619140625, 1.4022856950759888, 1.3422983884811401, 1.3717480897903442, 1.3983840942382812, 1.3755419254302979, 1.3972768783569336, 1.416441798210144, 1.4417731761932373, 1.4334731101989746, 1.427716851234436]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0245e901",
        "outputId": "2039647f-deef-4abd-bd18-843bc27fc642"
      },
      "source": [
        "\n",
        "# Predicting code for an image\n",
        "from tensorflow.keras.preprocessing import image\n",
        "# Please replace the brackets below with the location of your image which need to predict\n",
        "img_path = '/content/drive/My Drive/Nebulas/Dark/hubble386.jpeg'\n",
        "img = image.load_img(img_path, target_size=IMG_SIZE)\n",
        "img_array = image.img_to_array(img)\n",
        "img_batch = np.expand_dims(img_array, axis=0)\n",
        "img_preprocessed = preprocess_input(img_batch)\n",
        "prediction = model.predict(img_preprocessed)\n",
        "print(prediction)\n"
      ],
      "id": "0245e901",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[9.7099048e-01 3.4337512e-03 7.7143624e-05 2.5227439e-02 2.7117057e-04]]\n"
          ]
        }
      ]
    }
  ]
}